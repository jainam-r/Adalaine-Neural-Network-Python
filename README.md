# Adaline-Neural-Network-Python

## The reasoning behind the training process is still the same as the perceptron: each weight w(i,j) needs to be updated in such a way that it will increase the amount of correctly predicted outputs on the next iteration — we call this update value Δ(i,j). However, this update variable is calculated in a different way, using an algorithm known as the gradient descent.

## Inputs:
|  X1  |  X2  |
|------|------|
|  1   |   1  |
|  1   |  -1  |
|  -1  |   1  |
|  -1  |  -1  |

## The Logic gates implemented in this project for the following Bipolar Inputs are :
## AND, OR, XOR, NOR and NAND
